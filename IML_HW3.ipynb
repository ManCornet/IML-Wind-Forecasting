{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8531373-cd83-40d0-bbc2-28b641c208e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2.5vw; color:#a4342d; font-weight:bold;\">\n",
    "    Introduction to machine learning - Homework 3\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "<b>Authors</b>: <i>C. Bosch, M. Cornet & V. Mangeleer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7eac",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Initialization\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to initialize all the librairies needed and load the untouched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LIBRAIRIES --\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Allow notebook to plot in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTION --\n",
    "\n",
    "# Used to print a basic section title in terminal\n",
    "def section(title = \"UNKNOWN\"):\n",
    "\n",
    "    # Number of letters to determine section size\n",
    "    title_size = len(title)\n",
    "\n",
    "    # Section title boundaries\n",
    "    boundary  = \"-\"\n",
    "    for i in range(title_size + 1):\n",
    "        boundary += \"-\"\n",
    "    \n",
    "    # Printing section\n",
    "    print(boundary)\n",
    "    print(f\" {title} \")\n",
    "    print(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ORIGINAL DATASET --\n",
    "# The original dataset is contained in the \"data/original\" folder\n",
    "\n",
    "# Stores the original dataset\n",
    "dataset_original_X = []\n",
    "dataset_original_Y = []\n",
    "\n",
    "# Load the original dataset\n",
    "for i in range(1, 11):\n",
    "    dataset_original_X.append(pd.read_csv(f\"data/original/X_Zone_{i}.csv\"))\n",
    "    dataset_original_Y.append(pd.read_csv(f\"data/original/Y_Zone_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BASIC INFORMATION DATASET --\n",
    "\n",
    "# Loading X and Y dataset for the first wind turbine\n",
    "dataset_X1 = dataset_original_X[0]\n",
    "dataset_Y1 = dataset_original_Y[0]\n",
    "\n",
    "# Displaying their relative information\n",
    "section(\"WIND TURBINE 1 - X Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_X1.head())\n",
    "section(\"INFO\")\n",
    "dataset_X1.info()\n",
    "\n",
    "section(\"WIND TURBINE 1 - Y Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_Y1.head())\n",
    "section(\"INFO\")\n",
    "dataset_Y1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e84f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Exploring\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to gain some basic insight regarding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HISTOGRAM --\n",
    "\n",
    "# Extracting only relevant variables\n",
    "dataset_X1_relevant = dataset_X1[[\"U10\", \"U100\", \"V10\", \"V100\"]]\n",
    "dataset_Y1_relevant = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]   # /!\\ Removing test samples (y = -1) /!\\\n",
    "dataset_Y1_relevant = dataset_Y1_relevant[[\"TARGETVAR\"]]\n",
    "\n",
    "# Observing distributions\n",
    "dataset_X1_relevant.hist(bins = 60, figsize = (20, 15))\n",
    "dataset_Y1_relevant.hist(bins = 60, figsize = (15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0abd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- OBSERVING WIND vs POWER --\n",
    "\n",
    "# Removing test data\n",
    "dataset_X_clean = dataset_X1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "dataset_Y_clean = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "\n",
    "# Computing total wind speed\n",
    "u_wind   = dataset_X_clean[[\"U100\"]].to_numpy()\n",
    "v_wind   = dataset_X_clean[[\"V100\"]].to_numpy()\n",
    "wind_tot = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "# Retreiving power\n",
    "power = dataset_Y_clean[[\"TARGETVAR\"]].to_numpy()\n",
    "\n",
    "# To see more clearly, one sample out of 2 is removed\n",
    "for i in range(1):\n",
    "    wind_tot = wind_tot[1::2]\n",
    "    power    = power[1::2]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(wind_tot, power, s = 3)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Speed [m/s]\")\n",
    "plt.ylabel(\"Normalized Power [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c306f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Train & Test | DataLoader\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb17e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS : MEAN, VARIANCE, ZONAL AVERAGE SPEED AND TIME STEPS --\n",
    "#\n",
    "# Used to compute the mean and variance of a variable over some timeslices in the dataset\n",
    "def computeMeanVariance(datasets, \n",
    "                        variables = [\"U100\", \"V100\"],\n",
    "                        window    = 100,\n",
    "                        variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert window > 1, \"Window size must be greater than 1 to compute mean and var\"\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the variables whose mean and var must be computed\n",
    "        for v in variables:\n",
    "\n",
    "            # Retreiving data \n",
    "            data = d.loc[: , [v]].to_numpy()\n",
    "\n",
    "            # Stores mean and variance (1st and 2nd : mean = their value, var = 0 otherwise NAN problem while computation)\n",
    "            mean = [data[0][0], data[1][0]]\n",
    "            var  = [0, 0]\n",
    "\n",
    "            for i in range(2, len(data)):\n",
    "\n",
    "                # Start and end index for computation\n",
    "                index_start = i - window if i - window >= 0 else 0\n",
    "                index_end   = i - 1 if i - 1 >= 0 else 0\n",
    "\n",
    "                # Computing mean and variance (much faster using numpy variables)\n",
    "                mean.append(np.mean(data[index_start:index_end]))\n",
    "                var.append(np.var(data[index_start:index_end]))\n",
    "            \n",
    "            # Adding the new data to dataset\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to compute the instantenous mean and variance of a variable accross multiple datasets\n",
    "def computeZonalValue(datasets, \n",
    "                      variables = [\"U100\", \"V100\"],\n",
    "                      variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert len(datasets) > 1, \"To compute mean and var, at least 2 datasets are needed\"\n",
    "\n",
    "    # Looping over the variables whose mean and var must be computed\n",
    "    for v in variables:\n",
    "\n",
    "        # Number of samples\n",
    "        nb_samples = len(datasets[0])\n",
    "\n",
    "        # Stores all the different values in numpy matrix for efficient computation\n",
    "        data = np.zeros((nb_samples, len(datasets)))\n",
    "\n",
    "        # Retreiving all the corresponding data\n",
    "        for i, d in enumerate(datasets):\n",
    "            \n",
    "            # Squeeze is there to remove useless dimension (Ask Victor)\n",
    "            data[:, i] = np.squeeze(d.loc[: , [v]].to_numpy())\n",
    "\n",
    "        # Computing mean and variance (much faster using numpy variables)\n",
    "        mean = np.mean(data, axis = 1) # Axis = 1 to make mean over each row\n",
    "        var  = np.var(data, axis = 1)\n",
    "\n",
    "        # Adding new data to all the datasets\n",
    "        for d in datasets:\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to add the value taken by a given variable over the past samples\n",
    "def addPastTime(datasets,\n",
    "                variables = [\"U100\", \"V100\"],\n",
    "                window    = 3):\n",
    "    #\n",
    "    # Note from Victor\n",
    "    # This function was a pain in the ass to make ! Even I, am not sure why it works well :D\n",
    "    #\n",
    "    # Security\n",
    "    assert window > 0, \"Window size must be greater than 0 to add past samples\"\n",
    "\n",
    "    # Looping over the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Retrieving current data\n",
    "            data = d[[v]].to_numpy()\n",
    "\n",
    "            # Stores all the past results\n",
    "            former_data = np.zeros((len(data), window))\n",
    "\n",
    "            # Looping over the corresponding data\n",
    "            for j in range(len(data)):\n",
    "\n",
    "                # Start and end index for retreiving values\n",
    "                index_start = j - window if j - window >= 0 else 0\n",
    "                index_end   = j if j - 1 >= 0 else 0\n",
    "                \n",
    "                # Retrieve corresponding value\n",
    "                values = data[index_start:index_end]\n",
    "\n",
    "                # Fixing case where looking at starting indexes < window size\n",
    "                if len(values) != window:\n",
    "                    values = np.append(np.zeros((window - len(values), 1)), values)\n",
    "\n",
    "                # Placing the data (such that by reading left to right: t - 1, t - 2, t - 3, ...)\n",
    "                for k, val in enumerate(values):\n",
    "                        former_data[j][k] = val\n",
    "\n",
    "            # Addding past results in the dataset\n",
    "            for t in range(window):\n",
    "                d[f\"{v}_(t-{window - t})\"] = former_data[:, t]\n",
    "\n",
    "# Used to normalize the data of different variables\n",
    "def normalize(datasets,\n",
    "              norm_type = \"argmax\",\n",
    "              data_type = \"column\",\n",
    "              variables = [\"U100\", \"V100\"]):\n",
    "    \"\"\"\n",
    "    Documentation :\n",
    "        - norm_type (str) : argmax, mean\n",
    "            - Normalize using the argmax or by using the mean and std of the data\n",
    "        - data_type (str) : column, all\n",
    "            - Apply the normalization using norm_type computed either on a unique column or all the same columns\n",
    "    \"\"\"\n",
    "    # Security\n",
    "    assert norm_type in [\"argmax\", \"mean\"], \"Normalization types = argmax, mean\"\n",
    "    assert data_type in [\"column\", \"all\"] , \"Data types = column, all\"\n",
    "\n",
    "    # Initialization of the normalization variables\n",
    "    argmax, mean, std = list(), list(), list()\n",
    "\n",
    "    # 1 - Computing argmax or mean and std of all datasets\n",
    "    if data_type == \"all\":\n",
    "\n",
    "        # Looping over the different variables to normalize\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Initialization of the normalization variables\n",
    "            argmax_list, mean_list, std_list = list(), list(), list()\n",
    "\n",
    "            # Looping over all the datasets\n",
    "            for d in datasets:\n",
    "\n",
    "                # Retrieving currently observed data\n",
    "                current_data = d[[v]].to_numpy()\n",
    "\n",
    "                # Retrieving variables\n",
    "                argmax_list.append(np.max(np.abs(current_data)))\n",
    "                mean_list.append(np.mean(current_data))\n",
    "                std_list.append(np.std(current_data))\n",
    "\n",
    "            # Adding results\n",
    "            argmax.append(max(argmax_list))\n",
    "            mean.append(sum(mean_list)/len(mean_list))\n",
    "            std.append(sum(std_list)/len(std_list))\n",
    "    \n",
    "    # 2 - Normalization of the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "            \n",
    "            # Case 1 - Mean and std - Column\n",
    "            if norm_type == \"mean\" and data_type == \"column\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - np.mean(data))/np.std(data)\n",
    "\n",
    "            # Case 2 - Mean and std - All\n",
    "            elif norm_type == \"mean\" and data_type == \"all\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - mean[i])/std[i]\n",
    "            \n",
    "            # Case 3 - Argmax - Column\n",
    "            elif norm_type == \"argmax\" and data_type == \"column\":\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/np.max(data)\n",
    "\n",
    "            # Case 4 - Argmax - All\n",
    "            else:\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/argmax[i]\n",
    "\n",
    "# Used to remove specific columns from the dataset\n",
    "def remove(datasets, variables):\n",
    "\n",
    "    # Looping over all datasets and variables\n",
    "    for d in datasets:\n",
    "        for v in variables:\n",
    "\n",
    "            # Removing\n",
    "            d.drop(v, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER -- \n",
    "# This class has for purpose to handle the data and make our life easier ! \n",
    "#\n",
    "class dataLoader():\n",
    "    \n",
    "    # Initialization of the loader\n",
    "    def __init__(self, datasets_X, datasets_Y):\n",
    "\n",
    "        # Stores the original, transformed and final datasets\n",
    "        self.original_datasets_X    = datasets_X\n",
    "        self.original_datasets_Y    = datasets_Y\n",
    "        self.transformed_datasets_X = datasets_X\n",
    "        self.transformed_datasets_Y = datasets_Y\n",
    "        self.final_dataset_X        = None\n",
    "        self.final_dataset_Y        = None\n",
    "\n",
    "        # Used to know if datasets have been combined or not\n",
    "        self.isCombined = None\n",
    "\n",
    "    # Used to display the head of the transformed dataset (first set)\n",
    "    def showHeadTransformed(self):\n",
    "        section(\"Dataset - X - Transformed\")\n",
    "        print(self.transformed_datasets_X[0].head())\n",
    "        section(\"Dataset - Y - Transformed\")\n",
    "        print(self.transformed_datasets_Y[0].head())\n",
    "\n",
    "    # Used to split the final dataset into a train and test set (In test set, values for y are equal to -1)\n",
    "    def splitTrainTest(self, save = False, save_dir = \"new_data\"):\n",
    "\n",
    "        # Security\n",
    "        assert self.isCombined != None, \"You must first use self.finalize\"\n",
    "\n",
    "        # Case 1 - Datasets have been combined all together\n",
    "        if self.isCombined == True:\n",
    "            X_train = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            Y_train = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            X_test  = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] == -1]\n",
    "            Y_test  = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] == -1] # Not useful, I know !\n",
    "\n",
    "        # Case 2 - Datasets are still separated\n",
    "        if self.isCombined == False:\n",
    "            \n",
    "            X_train, Y_train, X_test, Y_test = list(), list(), list(), list()\n",
    "\n",
    "            # Looping over all the small datasets\n",
    "            for x, y in zip(self.final_dataset_X, self.final_dataset_Y):\n",
    "                X_train.append(x[y['TARGETVAR'] != -1])\n",
    "                Y_train.append(y[y['TARGETVAR'] != -1])\n",
    "                X_test.append(x[y['TARGETVAR'] == -1])\n",
    "                Y_test.append(y[y['TARGETVAR'] == -1])\n",
    "\n",
    "        # Be careful with the order\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "        \n",
    "    # Used to perfom final operation on dataset (Combining everything or storing them separately)\n",
    "    def finalization(self, dataset_type = \"combined\"):\n",
    "\n",
    "        # Security\n",
    "        assert dataset_type in [\"combined\", \"separated\"], \"The final dataset can either be of type combined or separated\"\n",
    "\n",
    "        # Case 1 - Combining into one big dataset\n",
    "        if dataset_type == \"combined\":\n",
    "            self.final_dataset_X = pd.concat(self.transformed_datasets_X)\n",
    "            self.final_dataset_Y = pd.concat(self.transformed_datasets_Y)\n",
    "            self.isCombined = True\n",
    "\n",
    "        # Case 2 - Separated datasets\n",
    "        else:\n",
    "            self.final_dataset_X = self.transformed_datasets_X\n",
    "            self.final_dataset_Y = self.transformed_datasets_Y\n",
    "            self.isCombined = False\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #                                    PIPELINES\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #\n",
    "    # List of functions available:\n",
    "    #\n",
    "    # - computeMeanVariance(datasets, variables = ≈, window = 100, variance  = True)\n",
    "    #\n",
    "    # - computeZonalValue(datasets, variables = [\"U100\", \"V100\"], variance  = True)\n",
    "    # \n",
    "    # - addPastTime(datasets, variables = [\"U100\", \"V100\"], window = 3):\n",
    "    #\n",
    "    # - normalize(datasets, norm_type = \"argmax\", data_type = \"column\", variables = [\"U100\", \"V100\"])\n",
    "    #\n",
    "    def pipeline(self, useMeanVariance = True, var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                       useZonal        = True, var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                       usePastTime     = True, var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON   = 3,\n",
    "                       useNormalize    = True, var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"argmax\", data_type = \"column\"):\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying the different transformations\n",
    "        if useNormalize:\n",
    "            normalize(dX, variables = var_NORM, norm_type = norm_type, data_type = data_type)\n",
    "        if useMeanVariance:\n",
    "            computeMeanVariance(dX, variables = var_MV, window = window_MV, variance = variance_MV)\n",
    "        if useZonal:\n",
    "            computeZonalValue(dX, variables = var_ZON, variance = variance_ZON)\n",
    "        if usePastTime:\n",
    "            addPastTime(dX, variables = var_PT, window = window_ZON)\n",
    "\n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521edde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (2) - ARGMAX ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_2 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_2.pipeline(norm_type = \"argmax\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_2.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_2, _, _, _ = loader_2.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_2             = data_X_2.corr()\n",
    "corr_2[np.abs(corr_2) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_2, cmap = \"YlGnBu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (3) - MEAN COLUMN -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_3 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_3.pipeline(norm_type = \"mean\", data_type = \"column\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_3.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_3, _, _, _ = loader_3.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_3             = data_X_3.corr()\n",
    "corr_3[np.abs(corr_3) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_3, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (4) - MEAN ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_4 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_4.pipeline(norm_type = \"mean\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_4.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_4, _, _, _ = loader_4.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_4             = data_X_4.corr()\n",
    "corr_4[np.abs(corr_4) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_4, cmap = \"Greens\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5741b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CORRELATION MATRIX COMPARISON --\n",
    "corr_21 = corr_2 - corr_1\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_21, cmap = \"Greens\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_23 = corr_2 - corr_3\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_23, cmap = \"BuPu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_24 = corr_2 - corr_4\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_24, cmap = \"YlGnBu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_31 = corr_3 - corr_1\n",
    "corr_31[np.abs(corr_31) < 0.2] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_31, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ea381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template\n",
    "\"\"\"\n",
    "loader.pipeline(useMeanVariance = True, var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                useZonal        = True, var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                usePastTime     = True, var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON = 3,\n",
    "                useNormalize    = True, var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"argmax\", data_type = \"column\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the loader\n",
    "loader = dataLoader(dataset_original_X, dataset_original_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 1 - ORIGINAL\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = False)\n",
    "loader.finalization()\n",
    "\n",
    "data_X0, submit_X0, data_Y0, submit_Y0 = loader.splitTrainTest()\n",
    "print(data_X0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 2 - INFLUENCE OF NORMALIZATION\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"argmax\", data_type = \"column\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X1, submit_X1, data_Y1, submit_Y1 = loader.splitTrainTest()\n",
    "print(data_X1.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"argmax\", data_type = \"all\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X2, submit_X2, data_Y2, submit_Y2 = loader.splitTrainTest()\n",
    "print(data_X2.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"mean\", data_type = \"column\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X3, submit_X3, data_Y3, submit_Y3 = loader.splitTrainTest()\n",
    "print(data_X3.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"mean\", data_type = \"all\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X4, submit_X4, data_Y4, submit_Y4 = loader.splitTrainTest()\n",
    "print(data_X4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Datasets --\n",
    "data_X_tot = [data_X0, data_X1, data_X2, data_X3, data_X4]\n",
    "data_Y_tot = [data_Y0, data_Y1, data_Y2, data_Y3, data_Y4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06df83",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Model - Creation of a pre-classifier\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to create a regression tree which allow us to determine wheter or not power will be produced.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11846a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CREATION OF THE DATASETS FOR PRE-CLASSIFIER --\n",
    "# \n",
    "# Contains the new datasets\n",
    "data_X_PRE = copy.deepcopy(data_X4)\n",
    "data_Y_PRE = copy.deepcopy(data_Y4)\n",
    "\n",
    "# Transforming into binary classification problem (0 = no power, 1 = power to predict)\n",
    "data_Y_PRE[data_Y_PRE[\"TARGETVAR\"] != 0] = 1\n",
    "\n",
    "# Removing useless information (apparently not that useless ahah)\n",
    "# data_X_PRE.drop([\"Day\", \"Month\", \"Year\", \"Hour\", \"ZONEID\"], inplace = True, axis = 1)\n",
    "\n",
    "# Final conversion\n",
    "data_X_PRE = data_X_PRE.to_numpy()\n",
    "data_Y_PRE = data_Y_PRE[\"TARGETVAR\"].to_numpy()\n",
    "\n",
    "# Generation of the train and test\n",
    "X_PRE_train, X_PRE_test, y_PRE_train, y_PRE_test = train_test_split(data_X_PRE, data_Y_PRE, test_size = 0.3, random_state = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924332b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TRAINING AND TESTING OF THE PRE-CLASSIFIER -- \n",
    "#\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialization of the model\n",
    "model_pre_classifier = tree.DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "# Fitting\n",
    "model_pre_classifier.fit(X_PRE_train, y_PRE_train)\n",
    "\n",
    "# Accuracy \n",
    "accuracy_pre_classifier = model_pre_classifier.score(X_PRE_test, y_PRE_test)\n",
    "\n",
    "print(f\"Pre classifier's accuracy [%] : {accuracy_pre_classifier * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model_pre_classifier.predict_proba(X_PRE_test)\n",
    "\n",
    "print(p[100:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fe0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CLASS OF ENSEMBLE MODEL --\n",
    "#\n",
    "# The goal of this class is to create our own predict function which is composed of 2 main steps:\n",
    "# - The pre-classifier's determines whether or not the wind is sufficiently high to have a power creation\n",
    "# - The model which predicts the power creation\n",
    "class ModelEnsemble():\n",
    "\n",
    "    # Initialization of the ensemble model\n",
    "    def __init__(self, model_prec_classifier, model_trained):\n",
    "        self.pre_classifier = model_prec_classifier\n",
    "        self.trained        = model_trained \n",
    "    \n",
    "    # Used to compute predictions\n",
    "    def predict(self, x, predict_treshold = 0.9):\n",
    "\n",
    "        # 1 - Determining if there is power or not \n",
    "        PC_results  = self.pre_classifier.predict_proba(x)\n",
    "\n",
    "        # 2 - Power predictions\n",
    "        MOD_results = self.trained.predict(x)\n",
    "\n",
    "        # 3 - Replacing predictions with pre-classifier results \n",
    "        for i, r in enumerate(PC_results):\n",
    "            MOD_results[i] = 0 if r[0] > predict_treshold else MOD_results[i]\n",
    "\n",
    "        return MOD_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e713392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TESTING\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Creation of the models\n",
    "model_simple = RandomForestRegressor(max_depth = 40, n_estimators = 20)\n",
    "\n",
    "X = data_X0.to_numpy()\n",
    "y = data_Y0[[\"TARGETVAR\"]].to_numpy().ravel()\n",
    "\n",
    "# Generation of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "# Fitting the model on current split\n",
    "model_simple.fit(X_train, y_train)\n",
    "\n",
    "# Creation of the complex model\n",
    "model_complexe = ModelEnsemble(model_pre_classifier, model_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing predictions\n",
    "pred_simple  = model_simple.predict(X_test)\n",
    "pred_complex = model_complexe.predict(X_test, predict_treshold = 0.9)\n",
    "\n",
    "# Computing and comparing accuracies\n",
    "acc_simple  = mean_squared_error(pred_simple, y_test,  squared = \"False\")\n",
    "acc_complex = mean_squared_error(pred_complex, y_test, squared = \"False\")\n",
    "\n",
    "print(acc_simple)\n",
    "print(acc_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1cb5f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Model - Training & Testing\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS --\n",
    "#\n",
    "# Used to compute a model's accuracy against different datasets\n",
    "def modelTesting(datasets_X, datasets_y, model, test_size = 0.3, random_state = 69):\n",
    "    \n",
    "    # Contains mean accuracy of the model against each dataset\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    # Looping over whole the different datasets\n",
    "    for X, y in zip(datasets_X, datasets_y):\n",
    "        \n",
    "        # Final conversion (Numpy and retrieving targets)\n",
    "        X = X.to_numpy()\n",
    "        y = y[[\"TARGETVAR\"]].to_numpy().ravel()\n",
    "\n",
    "        # Retrieving datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "\n",
    "        # Fitting the model on current split\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy_train.append(model.score(X_train, y_train))\n",
    "        accuracy_test.append(model.score(X_test, y_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test\n",
    "\n",
    "def modelPlotResults(parameters, acc_train, acc_test, xlabel = \"UNKNOWN\", param_name = \"UNKNOWN\", fontsize = 15, save_path = \"graphs/\"):\n",
    "\n",
    "    # 1 - Evolution of the test accuracy\n",
    "    plt.figure()\n",
    "    sns.set(font_scale = 2)\n",
    "\n",
    "    # Plotting evolution curve for a specific dataset with varying parameter value\n",
    "    for i, a in enumerate(acc_test):\n",
    "        plt.plot(parameters, [a_i * 100 for a_i in a], label = f\"Dataset n°{i}\", linewidth = 5)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize = fontsize)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.xlabel(xlabel, fontsize = fontsize)\n",
    "    plt.savefig(f\"{save_path}_1.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2 - Bar plot\n",
    "    acc_test_best  = []\n",
    "    acc_train_best = []\n",
    "    k_best         = []\n",
    "\n",
    "    # Looping over accuracies to find best results\n",
    "    for a1, a2 in zip(acc_train, acc_test):\n",
    "\n",
    "        # Finding best test accuracy\n",
    "        max_value = max(a2)\n",
    "        max_index = a2.index(max_value)\n",
    "\n",
    "        # Adding results\n",
    "        acc_test_best.append(max_value)\n",
    "        acc_train_best.append(a1[max_index])\n",
    "        k_best.append(parameters[max_index])\n",
    "\n",
    "    # Contains x-axis labels\n",
    "    x_ax_labels = [f\"Dataset n°{i} - {param_name} = {k_best[i]}\" for i in range(len(acc_train))]\n",
    "\n",
    "    # Used to make x-axis    \n",
    "    index = [i for i in range(len(acc_train))]\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure()\n",
    "    plt.bar([i - 0.2 for i in index], [a_i * 100 for a_i in acc_train_best], 0.4, label = \"Train\")\n",
    "    plt.bar([i + 0.2 for i in index], [a_i * 100 for a_i in acc_test_best], 0.4, label = \"Test\")\n",
    "    plt.xticks(index, x_ax_labels)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}_2.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653fb44",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">Multi-Layer-Perceptron</p>\n",
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0c2756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Deep Learning Librairies --\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94fd3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Neural Network (MLP) -- \n",
    "class MLP_PowerPrediction(nn.Module):\n",
    "\n",
    "    # Initalization of the model\n",
    "    def __init__(self, input_size, output_size = 1, device = \"None\"):\n",
    "\n",
    "        # Need to call the super to define child functions\n",
    "        super(MLP_PowerPrediction, self).__init__()\n",
    "\n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, 32, dtype = \"float64\")\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, output_size)\n",
    "\n",
    "    # Defining how data will flow through the network\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The Network is quite simple (A chain of fully connected and ReLu activation functions)\n",
    "        #\n",
    "        # First Layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second Layer\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Third Layer\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Fourth Layer\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86eb2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dataloader & Dataset (For the training of neural networks, one need batch of inputs) --\n",
    "class PowerDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "      self.x = x\n",
    "      self.y = y\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "      return (self.x[index], self.y[index])\n",
    "  \n",
    "  def __len__(self):\n",
    "      return self.x.shape[0]\n",
    "\n",
    "# Loading a dataset into a readable shape\n",
    "POWER_dataset  = DataLoader(PowerDataset(data_X0, data_Y0), batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5154900c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=str, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 36\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m POWER_dataset  \u001b[39m=\u001b[39m DataLoader(PowerDataset(POWER_data_X, POWER_data_Y), batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Intialization of the network\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m MLP_PowerPrediction(\u001b[39mlen\u001b[39;49m(POWER_data_X[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 36\u001b[0m in \u001b[0;36mMLP_PowerPrediction.__init__\u001b[0;34m(self, input_size, output_size, device)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39msuper\u001b[39m(MLP_PowerPrediction, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Defining the fully connected layers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(input_size, \u001b[39m32\u001b[39;49m, dtype \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mfloat64\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m32\u001b[39m, \u001b[39m16\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=str, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# -- Setting up -- \n",
    "#\n",
    "# Dataset \n",
    "POWER_data_X   = data_X0.to_numpy()\n",
    "POWER_data_Y   = data_Y0.to_numpy()\n",
    "POWER_dataset  = DataLoader(PowerDataset(POWER_data_X, POWER_data_Y), batch_size = 64)\n",
    "\n",
    "# Intialization of the network\n",
    "model = MLP_PowerPrediction(len(POWER_data_X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ccdebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(num_epochs/2), int(num_epochs * 3/4), int(num_epochs * 7/8)], gamma=0.1)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dde0fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 / 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 38\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Computing prediction and storing target\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m yhat  \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ytrue \u001b[39m=\u001b[39m y\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Computing loss\u001b[39;00m\n",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 38\u001b[0m in \u001b[0;36mMLP_PowerPrediction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# The Network is quite simple (A chain of fully connected and ReLu activation functions)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# First Layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X61sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Second Layer\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Display useful information over terminal (1)\n",
    "    print(\"Epoch : \", epoch + 1, \"/\", num_epochs)\n",
    "\n",
    "    # Retreiving a batch of data\n",
    "    for x, y in POWER_dataset:\n",
    "\n",
    "        # Reseting gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computing prediction and storing target\n",
    "        yhat  = model.forward(x)\n",
    "        ytrue = y\n",
    "\n",
    "        # Computing loss\n",
    "        loss = criterion(yhat, ytrue)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating epoch info ! Would be nice to upgrade it !\n",
    "        print(loss.item())\n",
    "\n",
    "\n",
    "    # Updating timing\n",
    "    # Just to make sure there is no overlap between progress bar and section\n",
    "    print(\" \")\n",
    "\n",
    "    # Updating the scheduler to update learning rate !\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10f146",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">KNeighborsRegressor</p>\n",
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results KNN -- \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "k_param = np.linspace(1, 100, 100, dtype = int)\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "knn_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "knn_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for k in k_param:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = KNeighborsRegressor(n_neighbors = k)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        knn_accuracy_train[i].append(acc_1)\n",
    "        knn_accuracy_test[i].append(acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPlotResults(k_param, knn_accuracy_train, knn_accuracy_test, \n",
    "                 xlabel = \"Number of neighbors - $k$ [-]\", fontsize = 30, \n",
    "                 param_name = \"k\", save_path = \"graphs/knn/knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca89775",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 100px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">Random Forest</p>\n",
    "<hr style=\"color:#a4342d; width: 100px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results Random Forest -- \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "max_depth = np.linspace(1, 60, 60)\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "rf_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "rf_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for d in max_depth:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = RandomForestRegressor(max_depth = d, n_estimators = 20)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        rf_accuracy_train[i].append(acc_1)\n",
    "        rf_accuracy_test[i].append(acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390066d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "modelPlotResults(max_depth, rf_accuracy_train, rf_accuracy_test, \n",
    "                 xlabel = \"Depth of the tree - $d$ [-]\", fontsize = 30, \n",
    "                 param_name = \"d\", save_path = \"graphs/rf/rf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6970a6b860234838f15385a694e4c719e6911821bb317b2dd9580ae5d017ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
