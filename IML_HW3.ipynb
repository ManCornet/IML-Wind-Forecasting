{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8531373-cd83-40d0-bbc2-28b641c208e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2.5vw; color:#a4342d; font-weight:bold;\">\n",
    "    Introduction to machine learning - Homework 3\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "<b>Authors</b>: <i>C. Bosch, M. Cornet & V. Mangeleer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7eac",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Initialization\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to initialize all the librairies needed and load the untouched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LIBRAIRIES --\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Allow notebook to plot in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTION --\n",
    "\n",
    "# Used to print a basic section title in terminal\n",
    "def section(title = \"UNKNOWN\"):\n",
    "\n",
    "    # Number of letters to determine section size\n",
    "    title_size = len(title)\n",
    "\n",
    "    # Section title boundaries\n",
    "    boundary  = \"-\"\n",
    "    for i in range(title_size + 1):\n",
    "        boundary += \"-\"\n",
    "    \n",
    "    # Printing section\n",
    "    print(boundary)\n",
    "    print(f\" {title} \")\n",
    "    print(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5da1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ORIGINAL DATASET --\n",
    "# The original dataset is contained in the \"data/original\" folder\n",
    "\n",
    "# Stores the original dataset\n",
    "dataset_original_X = []\n",
    "dataset_original_Y = []\n",
    "\n",
    "# Load the original dataset\n",
    "for i in range(1, 11):\n",
    "    dataset_original_X.append(pd.read_csv(f\"data/original/X_Zone_{i}.csv\"))\n",
    "    dataset_original_Y.append(pd.read_csv(f\"data/original/Y_Zone_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3e0c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      " WIND TURBINE 1 - X Dataset \n",
      "----------------------------\n",
      "------\n",
      " HEAD \n",
      "------\n",
      "   ZONEID       U10       V10      U100      V100  Day  Month  Year  Hour  \\\n",
      "0       1  0.116541 -0.253693  0.084184 -0.213789    1      1  2012     1   \n",
      "1       1  0.155554 -0.162680  0.115345 -0.132693    1      1  2012     2   \n",
      "2       1  0.170341 -0.062468  0.125952 -0.048266    1      1  2012     3   \n",
      "3       1  0.149248  0.007347  0.106940  0.009691    1      1  2012     4   \n",
      "4       1  0.128458  0.062183  0.090240  0.056152    1      1  2012     5   \n",
      "\n",
      "   U10_mean  ...  U10_(t-1)  V10_(t-3)  V10_(t-2)  V10_(t-1)  U100_(t-3)  \\\n",
      "0  0.116541  ...   0.000000   0.000000   0.000000   0.000000    0.000000   \n",
      "1  0.155554  ...   0.116541   0.000000   0.000000  -0.253693    0.000000   \n",
      "2  0.116541  ...   0.155554   0.000000  -0.253693  -0.162680    0.000000   \n",
      "3  0.136048  ...   0.170341  -0.253693  -0.162680  -0.062468    0.084184   \n",
      "4  0.147479  ...   0.149248  -0.162680  -0.062468   0.007347    0.115345   \n",
      "\n",
      "   U100_(t-2)  U100_(t-1)  V100_(t-3)  V100_(t-2)  V100_(t-1)  \n",
      "0    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
      "1    0.000000    0.084184    0.000000    0.000000   -0.213789  \n",
      "2    0.084184    0.115345    0.000000   -0.213789   -0.132693  \n",
      "3    0.115345    0.125952   -0.213789   -0.132693   -0.048266  \n",
      "4    0.125952    0.106940   -0.132693   -0.048266    0.009691  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "------\n",
      " INFO \n",
      "------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17544 entries, 0 to 17543\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ZONEID           17544 non-null  int64  \n",
      " 1   U10              17544 non-null  float64\n",
      " 2   V10              17544 non-null  float64\n",
      " 3   U100             17544 non-null  float64\n",
      " 4   V100             17544 non-null  float64\n",
      " 5   Day              17544 non-null  int64  \n",
      " 6   Month            17544 non-null  int64  \n",
      " 7   Year             17544 non-null  int64  \n",
      " 8   Hour             17544 non-null  int64  \n",
      " 9   U10_mean         17544 non-null  float64\n",
      " 10  U10_var          17544 non-null  float64\n",
      " 11  V10_mean         17544 non-null  float64\n",
      " 12  V10_var          17544 non-null  float64\n",
      " 13  U100_mean        17544 non-null  float64\n",
      " 14  U100_var         17544 non-null  float64\n",
      " 15  V100_mean        17544 non-null  float64\n",
      " 16  V100_var         17544 non-null  float64\n",
      " 17  U10_zonal_mean   17544 non-null  float64\n",
      " 18  U10_zonal_var    17544 non-null  float64\n",
      " 19  V10_zonal_mean   17544 non-null  float64\n",
      " 20  V10_zonal_var    17544 non-null  float64\n",
      " 21  U100_zonal_mean  17544 non-null  float64\n",
      " 22  U100_zonal_var   17544 non-null  float64\n",
      " 23  V100_zonal_mean  17544 non-null  float64\n",
      " 24  V100_zonal_var   17544 non-null  float64\n",
      " 25  U10_(t-3)        17544 non-null  float64\n",
      " 26  U10_(t-2)        17544 non-null  float64\n",
      " 27  U10_(t-1)        17544 non-null  float64\n",
      " 28  V10_(t-3)        17544 non-null  float64\n",
      " 29  V10_(t-2)        17544 non-null  float64\n",
      " 30  V10_(t-1)        17544 non-null  float64\n",
      " 31  U100_(t-3)       17544 non-null  float64\n",
      " 32  U100_(t-2)       17544 non-null  float64\n",
      " 33  U100_(t-1)       17544 non-null  float64\n",
      " 34  V100_(t-3)       17544 non-null  float64\n",
      " 35  V100_(t-2)       17544 non-null  float64\n",
      " 36  V100_(t-1)       17544 non-null  float64\n",
      "dtypes: float64(32), int64(5)\n",
      "memory usage: 5.0 MB\n",
      "----------------------------\n",
      " WIND TURBINE 1 - Y Dataset \n",
      "----------------------------\n",
      "------\n",
      " HEAD \n",
      "------\n",
      "   ZONEID  TARGETVAR  Day  Month  Year  Hour\n",
      "0       1   0.000000    1      1  2012     1\n",
      "1       1   0.054879    1      1  2012     2\n",
      "2       1   0.110234    1      1  2012     3\n",
      "3       1   0.165116    1      1  2012     4\n",
      "4       1   0.156940    1      1  2012     5\n",
      "------\n",
      " INFO \n",
      "------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17544 entries, 0 to 17543\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ZONEID     17544 non-null  int64  \n",
      " 1   TARGETVAR  17544 non-null  float64\n",
      " 2   Day        17544 non-null  int64  \n",
      " 3   Month      17544 non-null  int64  \n",
      " 4   Year       17544 non-null  int64  \n",
      " 5   Hour       17544 non-null  int64  \n",
      "dtypes: float64(1), int64(5)\n",
      "memory usage: 822.5 KB\n"
     ]
    }
   ],
   "source": [
    "# -- BASIC INFORMATION DATASET --\n",
    "\n",
    "# Loading X and Y dataset for the first wind turbine\n",
    "dataset_X1 = dataset_original_X[0]\n",
    "dataset_Y1 = dataset_original_Y[0]\n",
    "\n",
    "# Displaying their relative information\n",
    "section(\"WIND TURBINE 1 - X Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_X1.head())\n",
    "section(\"INFO\")\n",
    "dataset_X1.info()\n",
    "\n",
    "section(\"WIND TURBINE 1 - Y Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_Y1.head())\n",
    "section(\"INFO\")\n",
    "dataset_Y1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e84f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Exploring\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to gain some basic insight regarding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HISTOGRAM --\n",
    "\n",
    "# Extracting only relevant variables\n",
    "dataset_X1_relevant = dataset_X1[[\"U10\", \"U100\", \"V10\", \"V100\"]]\n",
    "dataset_Y1_relevant = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]   # /!\\ Removing test samples (y = -1) /!\\\n",
    "dataset_Y1_relevant = dataset_Y1_relevant[[\"TARGETVAR\"]]\n",
    "\n",
    "# Observing distributions\n",
    "dataset_X1_relevant.hist(bins = 60, figsize = (20, 15))\n",
    "dataset_Y1_relevant.hist(bins = 60, figsize = (15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0abd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- OBSERVING WIND vs POWER --\n",
    "\n",
    "# Removing test data\n",
    "dataset_X_clean = dataset_X1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "dataset_Y_clean = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "\n",
    "# Computing total wind speed\n",
    "u_wind   = dataset_X_clean[[\"U100\"]].to_numpy()\n",
    "v_wind   = dataset_X_clean[[\"V100\"]].to_numpy()\n",
    "wind_tot = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "# Retreiving power\n",
    "power = dataset_Y_clean[[\"TARGETVAR\"]].to_numpy()\n",
    "\n",
    "# To see more clearly, one sample out of 2 is removed\n",
    "for i in range(1):\n",
    "    wind_tot = wind_tot[1::2]\n",
    "    power    = power[1::2]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(wind_tot, power, s = 3)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Speed [m/s]\")\n",
    "plt.ylabel(\"Normalized Power [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c306f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Train & Test | DataLoader\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cb17e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS : MEAN, VARIANCE, ZONAL AVERAGE SPEED AND TIME STEPS --\n",
    "#\n",
    "# Used to compute the mean and variance of a variable over some timeslices in the dataset\n",
    "def computeMeanVariance(datasets, \n",
    "                        variables = [\"U100\"     , \"V100\"],\n",
    "                        names_m   = [\"U100_mean\", \"V100_mean\"],\n",
    "                        names_v   = [\"U100_var\" , \"V100_var\"],\n",
    "                        window    = 100,\n",
    "                        variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert len(variables) == len(names_m), \"Each variable should have a name for its mean\"\n",
    "    assert len(variables) == len(names_v), \"Each variable should have a name for its var\"\n",
    "    assert window > 1, \"Window size must be greater than 1 to compute mean and var\"\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the variables whose mean and var must be computed\n",
    "        for v, nm, nv in zip(variables, names_m, names_v):\n",
    "\n",
    "            # Retreiving data \n",
    "            data = d.loc[: , [v]].to_numpy()\n",
    "\n",
    "            # Stores mean and variance (1st and 2nd : mean = their value, var = 0 otherwise NAN problem while computation)\n",
    "            mean = [data[0][0], data[1][0]]\n",
    "            var  = [0, 0]\n",
    "\n",
    "            for i in range(2, len(data)):\n",
    "\n",
    "                # Start and end index for computation\n",
    "                index_start = i - window if i - window >= 0 else 0\n",
    "                index_end   = i - 1 if i - 1 >= 0 else 0\n",
    "\n",
    "                # Computing mean and variance (much faster using numpy variables)\n",
    "                mean.append(np.mean(data[index_start:index_end]))\n",
    "                var.append(np.var(data[index_start:index_end]))\n",
    "            \n",
    "            # Adding the new data to dataset\n",
    "            d[nm] = mean\n",
    "            if variance:\n",
    "                d[nv] = var\n",
    "\n",
    "# Used to compute the instantenous mean and variance of a variable accross multiple datasets\n",
    "def computeZonalValue(datasets, \n",
    "                      variables = [\"U100\"           , \"V100\"],\n",
    "                      names_m   = [\"U100_zonal_mean\", \"V100_zonal_mean\"],\n",
    "                      names_v   = [\"U100_zonal_var\" , \"V100_zonal_var\"],\n",
    "                      variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert len(variables) == len(names_m), \"Each variable should have a name for its zonal mean\"\n",
    "    assert len(variables) == len(names_v), \"Each variable should have a name for its zonal var\"\n",
    "    assert len(datasets) > 1, \"To compute mean and var, at least 2 datasets are needed\"\n",
    "\n",
    "    # Looping over the variables whose mean and var must be computed\n",
    "    for v, nm, nv in zip(variables, names_m, names_v):\n",
    "\n",
    "        # Number of samples\n",
    "        nb_samples = len(datasets[0])\n",
    "\n",
    "        # Stores all the different values in numpy matrix for efficient computation\n",
    "        data = np.zeros((nb_samples, len(datasets)))\n",
    "\n",
    "        # Retreiving all the corresponding data\n",
    "        for i, d in enumerate(datasets):\n",
    "            \n",
    "            # Squeeze is there to remove useless dimension (Ask Victor)\n",
    "            data[:, i] = np.squeeze(d.loc[: , [v]].to_numpy())\n",
    "\n",
    "        # Computing mean and variance (much faster using numpy variables)\n",
    "        mean = np.mean(data, axis = 1) # Axis = 1 to make mean over each row\n",
    "        var  = np.var(data, axis = 1)\n",
    "\n",
    "        # Adding new data to all the datasets\n",
    "        for d in datasets:\n",
    "            d[nm] = mean\n",
    "            if variance:\n",
    "                d[nv] = var\n",
    "\n",
    "# Used to add the value taken by a given variable over the past samples\n",
    "def addPastTime(datasets,\n",
    "                variables = [\"U100\", \"V100\"],\n",
    "                window    = 3):\n",
    "    #\n",
    "    # Note from Victor\n",
    "    # This function was a pain in the ass to make ! Even I, am not sure why it works well :D\n",
    "    #\n",
    "    # Security\n",
    "    assert window > 0, \"Window size must be greater than 0 to add past samples\"\n",
    "\n",
    "    # Looping over the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Retrieving current data\n",
    "            data = d[[v]].to_numpy()\n",
    "\n",
    "            # Stores all the past results\n",
    "            former_data = np.zeros((len(data), window))\n",
    "\n",
    "            # Looping over the corresponding data\n",
    "            for j in range(len(data)):\n",
    "\n",
    "                # Start and end index for retreiving values\n",
    "                index_start = j - window if j - window >= 0 else 0\n",
    "                index_end   = j if j - 1 >= 0 else 0\n",
    "                \n",
    "                # Retrieve corresponding value\n",
    "                values = data[index_start:index_end]\n",
    "\n",
    "                # Fixing case where looking at starting indexes < window size\n",
    "                if len(values) != window:\n",
    "                    values = np.append(np.zeros((window - len(values), 1)), values)\n",
    "\n",
    "                # Placing the data (such that by reading left to right: t - 1, t - 2, t - 3, ...)\n",
    "                for k, val in enumerate(values):\n",
    "                        former_data[j][k] = val\n",
    "\n",
    "            # Addding past results in the dataset\n",
    "            for t in range(window):\n",
    "                d[f\"{v}_(t-{window - t})\"] = former_data[:, t]\n",
    "\n",
    "# Used to normalize the data of different variables\n",
    "def normalize(datasets,\n",
    "              norm_type = \"argmax\",\n",
    "              data_type = \"column\",\n",
    "              variables = [\"U100\", \"V100\"]):\n",
    "    \"\"\"\n",
    "    Documentation :\n",
    "        - norm_type (str) : argmax, mean\n",
    "            - Normalize using the argmax or by using the mean and std of the data\n",
    "        - data_type (str) : column, all\n",
    "            - Apply the normalization using norm_type computed either on a unique column or all the same columns\n",
    "    \"\"\"\n",
    "    # Security\n",
    "    assert norm_type in [\"argmax\", \"mean\"], \"Normalization types = argmax, mean\"\n",
    "    assert data_type in [\"column\", \"all\"] , \"Data types = column, all\"\n",
    "\n",
    "    # Initialization of the normalization variables\n",
    "    argmax, mean, std = list(), list(), list()\n",
    "\n",
    "    # 1 - Computing argmax or mean and std of all datasets\n",
    "    if data_type == \"all\":\n",
    "\n",
    "        # Looping over the different variables to normalize\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Initialization of the normalization variables\n",
    "            argmax_list, mean_list, std_list = list(), list(), list()\n",
    "\n",
    "            # Looping over all the datasets\n",
    "            for d in datasets:\n",
    "\n",
    "                # Retrieving currently observed data\n",
    "                current_data = d[[v]].to_numpy()\n",
    "\n",
    "                # Retrieving variables\n",
    "                argmax_list.append(np.max(np.abs(current_data)))\n",
    "                mean_list.append(np.mean(current_data))\n",
    "                std_list.append(np.std(current_data))\n",
    "\n",
    "            # Adding results\n",
    "            argmax.append(max(argmax_list))\n",
    "            mean.append(sum(mean_list)/len(mean_list))\n",
    "            std.append(sum(std_list)/len(std_list))\n",
    "    \n",
    "    # 2 - Normalization of the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "            \n",
    "            # Case 1 - Mean and std - Column\n",
    "            if norm_type == \"mean\" and data_type == \"column\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - np.mean(data))/np.std(data)\n",
    "\n",
    "            # Case 2 - Mean and std - All\n",
    "            elif norm_type == \"mean\" and data_type == \"all\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - mean[i])/std[i]\n",
    "            \n",
    "            # Case 3 - Argmax - Column\n",
    "            elif norm_type == \"argmax\" and data_type == \"column\":\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/np.max(data)\n",
    "\n",
    "            # Case 4 - Argmax - All\n",
    "            else:\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/argmax[i]\n",
    "\n",
    "# Used to remove specific columns from the dataset\n",
    "def remove(datasets, variables):\n",
    "\n",
    "    # Looping over all datasets and variables\n",
    "    for d in datasets:\n",
    "        for v in variables:\n",
    "\n",
    "            # Removing\n",
    "            d.drop(v, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd96462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER -- \n",
    "# This class has for purpose to handle the data and make our life easier ! \n",
    "#\n",
    "class dataLoader():\n",
    "    \n",
    "    # Initialization of the loader\n",
    "    def __init__(self, datasets_X, datasets_Y):\n",
    "\n",
    "        # Stores the original, transformed and final datasets\n",
    "        self.original_datasets_X    = datasets_X\n",
    "        self.original_datasets_Y    = datasets_Y\n",
    "        self.transformed_datasets_X = datasets_X\n",
    "        self.transformed_datasets_Y = datasets_Y\n",
    "        self.final_dataset_X        = None\n",
    "        self.final_dataset_Y        = None\n",
    "\n",
    "        # Used to know if datasets have been combined or not\n",
    "        self.isCombined = None\n",
    "\n",
    "    # Used to display the head of the transformed dataset (first set)\n",
    "    def showHeadTransformed(self):\n",
    "        section(\"Dataset - X - Transformed\")\n",
    "        print(self.transformed_datasets_X[0].head())\n",
    "        section(\"Dataset - Y - Transformed\")\n",
    "        print(self.transformed_datasets_Y[0].head())\n",
    "\n",
    "    # Used to split the final dataset into a train and test set (In test set, values for y are equal to -1)\n",
    "    def splitTrainTest(self, save = False, save_dir = \"new_data\"):\n",
    "\n",
    "        # Security\n",
    "        assert self.isCombined != None, \"You must first use self.finalize\"\n",
    "\n",
    "        # Case 1 - Datasets have been combined all together\n",
    "        if self.isCombined == True:\n",
    "            X_train = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            Y_train = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            X_test  = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] == -1]\n",
    "            Y_test  = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] == -1] # Not useful, I know !\n",
    "\n",
    "        # Case 2 - Datasets are still separated\n",
    "        if self.isCombined == False:\n",
    "            \n",
    "            X_train, Y_train, X_test, Y_test = list(), list(), list(), list()\n",
    "\n",
    "            # Looping over all the small datasets\n",
    "            for x, y in zip(self.final_dataset_X, self.final_dataset_Y):\n",
    "                X_train.append(x[y['TARGETVAR'] != -1])\n",
    "                Y_train.append(y[y['TARGETVAR'] != -1])\n",
    "                X_test.append(x[y['TARGETVAR'] == -1])\n",
    "                Y_test.append(y[y['TARGETVAR'] == -1])\n",
    "\n",
    "        # Be careful with the order\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "        \n",
    "    # Used to perfom final operation on dataset (Combining everything or storing them separately)\n",
    "    def finalization(self, dataset_type = \"combined\"):\n",
    "\n",
    "        # Security\n",
    "        assert dataset_type in [\"combined\", \"separated\"], \"The final dataset can either be of type combined or separated\"\n",
    "\n",
    "        # Case 1 - Combining into one big dataset\n",
    "        if dataset_type == \"combined\":\n",
    "            self.final_dataset_X = pd.concat(self.transformed_datasets_X)\n",
    "            self.final_dataset_Y = pd.concat(self.transformed_datasets_Y)\n",
    "            self.isCombined = True\n",
    "\n",
    "        # Case 2 - Separated datasets\n",
    "        else:\n",
    "            self.final_dataset_X = self.transformed_datasets_X\n",
    "            self.final_dataset_Y = self.transformed_datasets_Y\n",
    "            self.isCombined = False\n",
    "\n",
    "    # Used to set as final dataset the original dataset\n",
    "    def original(self, combined = \"True\"): \n",
    "        \n",
    "        # Setting up for splitting\n",
    "        self.isCombined = None\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = self.original_datasets_X\n",
    "        dY = self.original_datasets_Y\n",
    "\n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #                                    PIPELINES\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #\n",
    "    # List of functions available:\n",
    "    #\n",
    "    # computeMeanVariance(datasets, variables = [\"U100\", \"V100\"], names_m = [\"U100_mean\", \"V100_mean\"], names_v = [\"U100_var\", \"V100_var\"],\n",
    "    #                     window = 100, variance  = True)\n",
    "    #\n",
    "    # computeZonalValue(datasets, variables = [\"U100\", \"V100\"], names_m = [\"U100_zonal_mean\", \"V100_zonal_mean\"], \n",
    "    #                   names_v = [\"U100_zonal_var\" , \"V100_zonal_var\"], variance  = True)\n",
    "    # \n",
    "    # addPastTime(datasets, variables = [\"U100\", \"V100\"], window = 3):\n",
    "    #\n",
    "    # normalize(datasets, norm_type = \"argmax\", data_type = \"column\", variables = [\"U100\", \"V100\"])\n",
    "    #\n",
    "    # TYPE 1 - This pipeline is used to create the correlation plots and gain intuition regarding which variable to use\n",
    "    def pipeline_corellation(self, norm_type = \"argmax\", data_type = \"column\"):\n",
    "\n",
    "        # Definition of the variables to improve\n",
    "        var      = [\"U10\", \"V10\", \"U100\", \"V100\"]\n",
    "        n_aver   = [\"U10_mean\", \"V10_mean\", \"U100_mean\", \"V100_mean\"]\n",
    "        n_var    = [\"U10_var\" , \"V10_var\", \"U100_var\" , \"V100_var\"]\n",
    "        n_z_aver = [\"U10_zonal_mean\", \"V10_zonal_mean\", \"U100_zonal_mean\", \"V100_zonal_mean\"]\n",
    "        n_z_var  = [\"U10_zonal_var\" , \"V10_zonal_var\", \"U100_zonal_var\" , \"V100_zonal_var\"]\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = self.original_datasets_X\n",
    "        dY = self.original_datasets_Y\n",
    "\n",
    "        # Applying transformations\n",
    "        normalize(dX, norm_type = norm_type, data_type = data_type, variables = var)\n",
    "        computeMeanVariance(dX, variables = var, names_m = n_aver,   names_v = n_var, window = 24, variance = True)\n",
    "        computeZonalValue(dX, variables = var, names_m = n_z_aver, names_v = n_z_var, variance = True)\n",
    "        addPastTime(dX, variables = var, window = 3)\n",
    "\n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None\n",
    "\n",
    "    # TYPE 2 - Normalizing all the data, removing year and day\n",
    "    def pipeline_1(self):\n",
    "\n",
    "        # Definition of the variables to improve\n",
    "        var      = [\"U10\", \"V10\", \"U100\", \"V100\"]\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying transformations\n",
    "        normalize(dX, norm_type = \"argmax\", data_type = \"column\", variables = var)\n",
    "        \n",
    "        # Removing useless information\n",
    "        #remove(dX, [\"Day\"])\n",
    "        \n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None\n",
    "\n",
    "    # TYPE 2 - Normalizing all the data, removing year and day\n",
    "    def pipeline_2(self, variance = True):\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying transformations\n",
    "        normalize(dX, norm_type = \"argmax\", data_type = \"column\")\n",
    "        \n",
    "        # Removing useless information\n",
    "        #remove(dX, [\"Day\"])\n",
    "        computeMeanVariance(dX, window = 24, variance  = variance)\n",
    "        \n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d941d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (1) - ARGMAX COLUMN -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_1 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_1.pipeline_corellation(norm_type = \"argmax\", data_type = \"column\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_1.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_1, _, _, _ = loader_1.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_1             = data_X_1.corr()\n",
    "corr_1[np.abs(corr_1) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_1, cmap = \"Blues\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521edde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (2) - ARGMAX ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_2 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_2.pipeline_corellation(norm_type = \"argmax\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_2.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_2, _, _, _ = loader_2.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_2             = data_X_2.corr()\n",
    "corr_2[np.abs(corr_2) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_2, cmap = \"YlGnBu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (3) - MEAN COLUMN -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_3 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_3.pipeline_corellation(norm_type = \"mean\", data_type = \"column\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_3.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_3, _, _, _ = loader_3.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_3             = data_X_3.corr()\n",
    "corr_3[np.abs(corr_3) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_3, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (4) - MEAN ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_4 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_4.pipeline_corellation(norm_type = \"mean\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_4.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_4, _, _, _ = loader_4.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_4             = data_X_4.corr()\n",
    "corr_4[np.abs(corr_4) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_4, cmap = \"Greens\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5741b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CORRELATION MATRIX COMPARISON --\n",
    "corr_21 = corr_2 - corr_1\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_21, cmap = \"Greens\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_23 = corr_2 - corr_3\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_23, cmap = \"BuPu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_24 = corr_2 - corr_4\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_24, cmap = \"YlGnBu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_31 = corr_3 - corr_1\n",
    "corr_31[np.abs(corr_31) < 0.2] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_31, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1cb5f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Model - Training & Testing\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95edf07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ZONEID       U10       V10      U100      V100  Day  Month  Year  Hour\n",
      "0       1  2.124600 -2.681966  2.864280 -3.666076    1      1  2012     1\n",
      "1       1  2.521695 -1.796960  3.344859 -2.464761    1      1  2012     2\n",
      "2       1  2.672210 -0.822516  3.508448 -1.214093    1      1  2012     3\n",
      "3       1  2.457504 -0.143642  3.215233 -0.355546    1      1  2012     4\n",
      "4       1  2.245898  0.389576  2.957678  0.332701    1      1  2012     5\n",
      "   ZONEID       U10       V10      U100      V100  Month  Year  Hour\n",
      "0       1  0.191112 -0.282051  0.168602 -0.256111      1  2012     1\n",
      "1       1  0.226832 -0.188979  0.196891 -0.172187      1  2012     2\n",
      "2       1  0.240371 -0.086501  0.206520 -0.084816      1  2012     3\n",
      "3       1  0.221058 -0.015106  0.189260 -0.024838      1  2012     4\n",
      "4       1  0.202023  0.040970  0.174100  0.023242      1  2012     5\n",
      "   ZONEID       U10       V10      U100      V100  Month  Year  Hour  \\\n",
      "0       1  2.124600 -2.681966  0.168602 -0.256111      1  2012     1   \n",
      "1       1  2.521695 -1.796960  0.196891 -0.172187      1  2012     2   \n",
      "2       1  2.672210 -0.822516  0.206520 -0.084816      1  2012     3   \n",
      "3       1  2.457504 -0.143642  0.189260 -0.024838      1  2012     4   \n",
      "4       1  2.245898  0.389576  0.174100  0.023242      1  2012     5   \n",
      "\n",
      "   U100_mean  V100_mean  \n",
      "0   0.168602  -0.256111  \n",
      "1   0.196891  -0.172187  \n",
      "2   0.168602  -0.256111  \n",
      "3   0.182746  -0.214149  \n",
      "4   0.190671  -0.171038  \n",
      "   ZONEID       U10       V10      U100      V100  Month  Year  Hour  \\\n",
      "0       1  2.124600 -2.681966  0.168602 -0.256111      1  2012     1   \n",
      "1       1  2.521695 -1.796960  0.196891 -0.172187      1  2012     2   \n",
      "2       1  2.672210 -0.822516  0.206520 -0.084816      1  2012     3   \n",
      "3       1  2.457504 -0.143642  0.189260 -0.024838      1  2012     4   \n",
      "4       1  2.245898  0.389576  0.174100  0.023242      1  2012     5   \n",
      "\n",
      "   U100_mean  U100_var  V100_mean  V100_var  \n",
      "0   0.168602  0.000000  -0.256111  0.000000  \n",
      "1   0.196891  0.000000  -0.172187  0.000000  \n",
      "2   0.168602  0.000000  -0.256111  0.000000  \n",
      "3   0.182746  0.000200  -0.214149  0.001761  \n",
      "4   0.190671  0.000259  -0.171038  0.004891  \n"
     ]
    }
   ],
   "source": [
    "# -- Creation of datasets\n",
    "\n",
    "# 1 - Original data\n",
    "loader_0 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader_0.original()\n",
    "loader_0.finalization()\n",
    "data_X0, submit_X0, data_Y0, submit_Y0 = loader_0.splitTrainTest()\n",
    "\n",
    "print(data_X0.head())\n",
    "\n",
    "# 2 - Different version\n",
    "loader_1 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader_1.pipeline_1()\n",
    "loader_1.finalization()\n",
    "data_X1, submit_X1, data_Y1, submit_Y1 = loader_1.splitTrainTest()\n",
    "\n",
    "print(data_X1.head())\n",
    "\n",
    "loader_2 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader_2.pipeline_2(variance = False)\n",
    "loader_2.finalization()\n",
    "data_X2, submit_X2, data_Y2, submit_Y2 = loader_2.splitTrainTest()\n",
    "\n",
    "print(data_X2.head())\n",
    "\n",
    "loader_3 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader_3.pipeline_2(variance = True)\n",
    "loader_3.finalization()\n",
    "data_X3, submit_X3, data_Y3, submit_Y3 = loader_3.splitTrainTest()\n",
    "\n",
    "print(data_X3.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5933e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Fitted - RF\n",
      "Fitted - RF, KNN\n",
      "Computing accuracy...\n",
      "KNN - Accuracy - Dataset n°0= 0.8377532991030915\n",
      "RF  - Accuracy - Dataset n°0= 0.8636027853314285\n",
      "Fitting...\n",
      "Fitted - RF\n",
      "Fitted - RF, KNN\n",
      "Computing accuracy...\n",
      "KNN - Accuracy - Dataset n°1= 0.7984708045246564\n",
      "RF  - Accuracy - Dataset n°1= 0.7375874745999287\n",
      "Fitting...\n",
      "Fitted - RF\n",
      "Fitted - RF, KNN\n",
      "Computing accuracy...\n",
      "KNN - Accuracy - Dataset n°2= 0.8357548959048624\n",
      "RF  - Accuracy - Dataset n°2= 0.7436508119431772\n",
      "Fitting...\n",
      "Fitted - RF\n",
      "Fitted - RF, KNN\n",
      "Computing accuracy...\n",
      "KNN - Accuracy - Dataset n°3= 0.8398329826109148\n",
      "RF  - Accuracy - Dataset n°3= 0.7436819866989951\n"
     ]
    }
   ],
   "source": [
    "# -- Training and experimenting --\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "\n",
    "# Contains all the datasets\n",
    "data_X_tot = [data_X0, data_X1, data_X2, data_X3]\n",
    "data_Y_tot = [data_Y0, data_Y1, data_Y2, data_Y3]\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Testing models on all the datasets and comparing them\n",
    "for X, Y in zip(data_X_tot, data_Y_tot):\n",
    "\n",
    "    # Creation of the sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Initialization of the model\n",
    "    model_1 = RandomForestRegressor(max_depth = 20, n_estimators = 50)\n",
    "    model_2 = KNeighborsRegressor(n_neighbors = 20)\n",
    "\n",
    "    # Fitting the data\n",
    "    print(\"Fitting...\")\n",
    "    model_1.fit(X_train, y_train)\n",
    "    print(\"Fitted - RF\")\n",
    "    model_2.fit(X_train, y_train)\n",
    "    print(\"Fitted - RF, KNN\")\n",
    "\n",
    "    # Computing accuracy\n",
    "    print(\"Computing accuracy...\")\n",
    "    accuracy_1 = model_1.score(X_test, y_test)\n",
    "    accuracy_2 = model_2.score(X_test, y_test)\n",
    "\n",
    "    # Results\n",
    "    print(f\"KNN - Accuracy - Dataset n°{index}= {accuracy_1}\")\n",
    "    print(f\"RF  - Accuracy - Dataset n°{index}= {accuracy_2}\")\n",
    "    index += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6b6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6970a6b860234838f15385a694e4c719e6911821bb317b2dd9580ae5d017ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
